{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4f/xq7119ld627_2wyd6dkmwl0m0000gn/T/ipykernel_89011/1828018844.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from nltk import word_tokenize\n",
    "from nltk.tag import CRFTagger\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anis = pd.read_csv('https://raw.github.com/ghazafm/SocialMediaSentiment/main/preprocessing/Training/data/raw/anis.csv')\n",
    "df_prabowo = pd.read_csv('https://raw.github.com/ghazafm/SocialMediaSentiment/main/preprocessing/Training/data/raw/prabowo.csv')\n",
    "df_ganjar = pd.read_csv('https://raw.github.com/ghazafm/SocialMediaSentiment/main/preprocessing/Training/data/raw/ganjar.csv')\n",
    "\n",
    "df_anis.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "df_prabowo.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "df_ganjar.drop('Unnamed: 0',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anis.drop_duplicates(inplace=True)\n",
    "df_prabowo.drop_duplicates(inplace=True)\n",
    "df_ganjar.drop_duplicates(inplace=True)\n",
    "df_anis.dropna(inplace=True)\n",
    "df_prabowo.dropna(inplace=True)\n",
    "df_ganjar.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_str_index(word,token):\n",
    "    temp = [0]\n",
    "    for j in word:    \n",
    "        if not j == temp[-1]:\n",
    "            temp.append(j)\n",
    "    temp.remove(0)\n",
    "    temp = ''.join(temp)\n",
    "    return temp\n",
    "\n",
    "\n",
    "def clear_double(data,token=False):\n",
    "    temp = []\n",
    "    data = data.split()\n",
    "    for word in data:\n",
    "        temp.append(remove_str_index(word,token))    \n",
    "    if token:\n",
    "        return temp\n",
    "    else:\n",
    "        return ' '.join(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anis['no_double'] = df_anis['Tweet'].apply(clear_double)\n",
    "df_prabowo['no_double'] = df_prabowo['Tweet'].apply(clear_double)\n",
    "df_ganjar['no_double'] = df_ganjar['Tweet'].apply(clear_double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "alay = pd.read_csv('https://raw.githubusercontent.com/nasalsabila/kamus-alay/master/colloquial-indonesian-lexicon.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "alay = dict(zip(alay['slang'], alay['formal']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cek_alay(word, alay):\n",
    "    return alay.get(word, word)\n",
    "\n",
    "\n",
    "def clear_alay(data):\n",
    "    words = str(data)\n",
    "    words = words.split()\n",
    "    cleared_words = [cek_alay(word, alay) for word in words]\n",
    "    return ' '.join(cleared_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anis['no_alay'] = df_anis['no_double'].apply(clear_alay)\n",
    "df_prabowo['no_alay'] = df_prabowo['no_double'].apply(clear_alay)\n",
    "df_ganjar['no_alay'] = df_ganjar['no_double'].apply(clear_alay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    text = word_tokenize(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = CRFTagger()\n",
    "ct.set_model_file('all_indo_man_tag_corpus_model.crf.tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_anis = df_anis['no_alay'].apply(tokenizer)\n",
    "tokenize_prabowo = df_prabowo['no_alay'].apply(tokenizer)\n",
    "tokenize_ganjar = df_ganjar['no_alay'].apply(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anis['pos'] = ct.tag_sents(tokenize_anis)\n",
    "df_prabowo['pos'] = ct.tag_sents(tokenize_prabowo)\n",
    "df_ganjar['pos'] = ct.tag_sents(tokenize_ganjar)\n",
    "\n",
    "tokenize_anis = df_anis['pos']\n",
    "tokenize_prabowo = df_prabowo['pos']\n",
    "tokenize_ganjar = df_ganjar['pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>no_double</th>\n",
       "      <th>no_alay</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>info  anies presiden</td>\n",
       "      <td>Positive</td>\n",
       "      <td>info anies presiden</td>\n",
       "      <td>info anies presiden</td>\n",
       "      <td>[(info, NN), (anies, NN), (presiden, NN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>politisi partai gerindra sandiaga uno menjawab...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>politisi partai gerindra sandiaga uno menjawab...</td>\n",
       "      <td>politisi partai gerindra sandiaga uno menjawab...</td>\n",
       "      <td>[(politisi, NN), (partai, NN), (gerindra, NN),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lanjut pak anies kita kawal sampai jadi presiden</td>\n",
       "      <td>Positive</td>\n",
       "      <td>lanjut pak anies kita kawal sampai jadi presiden</td>\n",
       "      <td>lanjut pak anies kita kawal sampai jadi presiden</td>\n",
       "      <td>[(lanjut, VB), (pak, NN), (anies, NN), (kita, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>semoga allah swt menyelamatkan bangsa dan nega...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>semoga alah swt menyelamatkan bangsa dan negar...</td>\n",
       "      <td>semoga alah swt menyelamatkan bangsa dan negar...</td>\n",
       "      <td>[(semoga, SC), (alah, VB), (swt, NN), (menyela...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chotimah kasian ya pa anies makanya sudah teka...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>chotimah kasian ya pa anies makanya sudah teka...</td>\n",
       "      <td>chotimah kasihan ya apa anies makanya sudah te...</td>\n",
       "      <td>[(chotimah, NN), (kasihan, NN), (ya, NN), (apa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>tidak ada gejolak sara selama membangun pks pu...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>tidak ada gejolak sara selama membangun pks pu...</td>\n",
       "      <td>tidak ada gejolak sara selama membangun pks pu...</td>\n",
       "      <td>[(tidak, NEG), (ada, VB), (gejolak, NN), (sara...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>ubedilah mahfud md otak di balik perppu ciptak...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>ubedilah mahfud md otak di balik perpu ciptake...</td>\n",
       "      <td>ubedilah mahfud md otak di balik perpu ciptake...</td>\n",
       "      <td>[(ubedilah, NN), (mahfud, FW), (md, FW), (otak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>my presiden mranies</td>\n",
       "      <td>Negative</td>\n",
       "      <td>my presiden mranies</td>\n",
       "      <td>my presiden mranies</td>\n",
       "      <td>[(my, FW), (presiden, FW), (mranies, FW)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>pa anies presiden</td>\n",
       "      <td>Negative</td>\n",
       "      <td>pa anies presiden</td>\n",
       "      <td>apa anies presiden</td>\n",
       "      <td>[(apa, WH), (anies, NN), (presiden, NN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>siapakah yg anda pilih presiden anies baswedan...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>siapakah yg anda pilih presiden anies baswedan...</td>\n",
       "      <td>siapakah yang anda pilih presiden anies baswed...</td>\n",
       "      <td>[(siapakah, NN), (yang, SC), (anda, PRP), (pil...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8913 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Tweet     label  \\\n",
       "0                                  info  anies presiden  Positive   \n",
       "1     politisi partai gerindra sandiaga uno menjawab...  Positive   \n",
       "2      lanjut pak anies kita kawal sampai jadi presiden  Positive   \n",
       "3     semoga allah swt menyelamatkan bangsa dan nega...  Positive   \n",
       "4     chotimah kasian ya pa anies makanya sudah teka...  Positive   \n",
       "...                                                 ...       ...   \n",
       "9995  tidak ada gejolak sara selama membangun pks pu...  Negative   \n",
       "9996  ubedilah mahfud md otak di balik perppu ciptak...  Negative   \n",
       "9997                                my presiden mranies  Negative   \n",
       "9998                                  pa anies presiden  Negative   \n",
       "9999  siapakah yg anda pilih presiden anies baswedan...  Negative   \n",
       "\n",
       "                                              no_double  \\\n",
       "0                                   info anies presiden   \n",
       "1     politisi partai gerindra sandiaga uno menjawab...   \n",
       "2      lanjut pak anies kita kawal sampai jadi presiden   \n",
       "3     semoga alah swt menyelamatkan bangsa dan negar...   \n",
       "4     chotimah kasian ya pa anies makanya sudah teka...   \n",
       "...                                                 ...   \n",
       "9995  tidak ada gejolak sara selama membangun pks pu...   \n",
       "9996  ubedilah mahfud md otak di balik perpu ciptake...   \n",
       "9997                                my presiden mranies   \n",
       "9998                                  pa anies presiden   \n",
       "9999  siapakah yg anda pilih presiden anies baswedan...   \n",
       "\n",
       "                                                no_alay  \\\n",
       "0                                   info anies presiden   \n",
       "1     politisi partai gerindra sandiaga uno menjawab...   \n",
       "2      lanjut pak anies kita kawal sampai jadi presiden   \n",
       "3     semoga alah swt menyelamatkan bangsa dan negar...   \n",
       "4     chotimah kasihan ya apa anies makanya sudah te...   \n",
       "...                                                 ...   \n",
       "9995  tidak ada gejolak sara selama membangun pks pu...   \n",
       "9996  ubedilah mahfud md otak di balik perpu ciptake...   \n",
       "9997                                my presiden mranies   \n",
       "9998                                 apa anies presiden   \n",
       "9999  siapakah yang anda pilih presiden anies baswed...   \n",
       "\n",
       "                                                    pos  \n",
       "0             [(info, NN), (anies, NN), (presiden, NN)]  \n",
       "1     [(politisi, NN), (partai, NN), (gerindra, NN),...  \n",
       "2     [(lanjut, VB), (pak, NN), (anies, NN), (kita, ...  \n",
       "3     [(semoga, SC), (alah, VB), (swt, NN), (menyela...  \n",
       "4     [(chotimah, NN), (kasihan, NN), (ya, NN), (apa...  \n",
       "...                                                 ...  \n",
       "9995  [(tidak, NEG), (ada, VB), (gejolak, NN), (sara...  \n",
       "9996  [(ubedilah, NN), (mahfud, FW), (md, FW), (otak...  \n",
       "9997          [(my, FW), (presiden, FW), (mranies, FW)]  \n",
       "9998           [(apa, WH), (anies, NN), (presiden, NN)]  \n",
       "9999  [(siapakah, NN), (yang, SC), (anda, PRP), (pil...  \n",
       "\n",
       "[8913 rows x 5 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_anis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_meaning = [\n",
    "    'jadi', 'menjadi', 'bapak', 'kalau', 'rakyat', 'siapa', \n",
    "    'apa', 'orang', 'bakal', 'sama', 'pasang', 'jelang', 'tahun', 'hari', \n",
    "    'bersama', 'mau', 'tetap', 'buat', 'for', 'bukan', 'semua', \n",
    "    'terus', 'si', 'inilah', 'kan', 'tak', 'banyak', 'meski', 'lebih', 'keputusan', \n",
    "    'final', 'paling', 'hasil', 'umum', 'tepat', 'tersebut', 'total', 'klik', 'capres', \n",
    "    'pilih', 'pemilihan', 'terpilih', 'survei', 'survey', 'pemilu', 'terkait', 'fahnoor', \n",
    "    'nan', 'calon', 'pilpres', 'resmi', 'cocok', 'politik', 'ribuan', 'ratusan', 'nama','maju',\n",
    "    'hut', 'dapat', 'semoga', 'beliau', 'besar', 'makin', 'layak', 'partai', 'mendukung', 'dukung', \n",
    "    'dukungan', 'gubernur', 'masyarakat', 'warga','presiden','ri','inismyname','pilpres','nan','calon','indonesia','survei','survey','pemilu',\n",
    "    'aa','aah','aak','aan'\n",
    "]\n",
    "\n",
    "name = [\n",
    "    'inismyname', 'indonesia', 'rosiade', 'joko', 'jokowi', 'widodo', 'ridwan', 'kamil', 'rosiade', \n",
    "    'thohir', 'mujani', 'erick', 'saiful', 'chotimah', 'ahy', 'bukan', 'aniesahy', 'ahmad', \n",
    "    'pks', 'pdip', 'jawa', 'puan', 'maharani', 'pan', 'jateng', 'tengah', 'megawati', 'ppp', \n",
    "    'rasyid', 'gerindra', 'nasdem', 'demokrat', 'pkb', 'allah', #maaffff,\n",
    "    'anis', 'anies', 'baswedan', 'prabowo', 'subianto', 'ganjar', 'pranowo','fahnoor', 'amien','sandiaga',\n",
    "    'chotimah', 'uno','aanies'\n",
    "]\n",
    "def clean_manual(data,token=False):\n",
    "    temp = []\n",
    "    if token:\n",
    "        for tup in data:\n",
    "            if tup[0] in double_meaning or tup[0] in name:\n",
    "                continue\n",
    "            temp.append(tup)\n",
    "        return temp\n",
    "    else:\n",
    "        for tup in data:\n",
    "            if tup[0] in double_meaning or tup[0] in name:\n",
    "                continue\n",
    "            temp.append(tup[0])\n",
    "    temp = ' '.join(temp)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anis['clean_manual'] = tokenize_anis.apply(clean_manual)\n",
    "df_prabowo['clean_manual'] = tokenize_prabowo.apply(clean_manual)\n",
    "df_ganjar['clean_manual'] = tokenize_ganjar.apply(clean_manual)\n",
    "\n",
    "\n",
    "tokenize_anis = tokenize_anis.apply(clean_manual,token=True)\n",
    "tokenize_prabowo = tokenize_prabowo.apply(clean_manual,token=True)\n",
    "tokenize_ganjar = tokenize_ganjar.apply(clean_manual,token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "def stemming(data,token=False):\n",
    "   temp = []\n",
    "   last = ''\n",
    "   for tup in data:\n",
    "      tup_temp = stemmer.stem(tup[0])\n",
    "      don = [tup_temp,tup[1]]\n",
    "      if token:\n",
    "         temp.append(don)\n",
    "         last = temp\n",
    "      else:\n",
    "         temp.append(tup_temp)\n",
    "         last = ' '.join(temp)\n",
    "   return last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_anis[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstemmed\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtokenize_anis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstemming\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m df_prabowo[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstemmed\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenize_prabowo\u001b[38;5;241m.\u001b[39mapply(stemming)\n\u001b[1;32m      3\u001b[0m df_ganjar[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstemmed\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenize_ganjar\u001b[38;5;241m.\u001b[39mapply(stemming)\n",
      "File \u001b[0;32m~/miniconda3/envs/bismillah/lib/python3.10/site-packages/pandas/core/series.py:4904\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4769\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4770\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4771\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4776\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4777\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4778\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4779\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4780\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4895\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4896\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4898\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4902\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4903\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4904\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bismillah/lib/python3.10/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bismillah/lib/python3.10/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/miniconda3/envs/bismillah/lib/python3.10/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bismillah/lib/python3.10/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[17], line 7\u001b[0m, in \u001b[0;36mstemming\u001b[0;34m(data, token)\u001b[0m\n\u001b[1;32m      5\u001b[0m last \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tup \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[0;32m----> 7\u001b[0m    tup_temp \u001b[38;5;241m=\u001b[39m \u001b[43mstemmer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m    don \u001b[38;5;241m=\u001b[39m [tup_temp,tup[\u001b[38;5;241m1\u001b[39m]]\n\u001b[1;32m      9\u001b[0m    \u001b[38;5;28;01mif\u001b[39;00m token:\n",
      "File \u001b[0;32m~/miniconda3/envs/bismillah/lib/python3.10/site-packages/Sastrawi/Stemmer/CachedStemmer.py:20\u001b[0m, in \u001b[0;36mCachedStemmer.stem\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     18\u001b[0m     stems\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache\u001b[38;5;241m.\u001b[39mget(word))\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 20\u001b[0m     stem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelegatedStemmer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache\u001b[38;5;241m.\u001b[39mset(word, stem)\n\u001b[1;32m     22\u001b[0m     stems\u001b[38;5;241m.\u001b[39mappend(stem)\n",
      "File \u001b[0;32m~/miniconda3/envs/bismillah/lib/python3.10/site-packages/Sastrawi/Stemmer/Stemmer.py:27\u001b[0m, in \u001b[0;36mStemmer.stem\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     24\u001b[0m stems \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words:\n\u001b[0;32m---> 27\u001b[0m     stems\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstem_word\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(stems)\n",
      "File \u001b[0;32m~/miniconda3/envs/bismillah/lib/python3.10/site-packages/Sastrawi/Stemmer/Stemmer.py:36\u001b[0m, in \u001b[0;36mStemmer.stem_word\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstem_plural_word(word)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstem_singular_word\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bismillah/lib/python3.10/site-packages/Sastrawi/Stemmer/Stemmer.py:84\u001b[0m, in \u001b[0;36mStemmer.stem_singular_word\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Stem a singular word to its common stem form.\"\"\"\u001b[39;00m\n\u001b[1;32m     83\u001b[0m context \u001b[38;5;241m=\u001b[39m Context(word, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdictionary, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisitor_provider)\n\u001b[0;32m---> 84\u001b[0m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m context\u001b[38;5;241m.\u001b[39mresult\n",
      "File \u001b[0;32m~/miniconda3/envs/bismillah/lib/python3.10/site-packages/Sastrawi/Stemmer/Context/Context.py:37\u001b[0m, in \u001b[0;36mContext.execute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute stemming process; the result can be retrieved with result\"\"\"\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m#step 1 - 5\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_stemming_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m#step 6\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdictionary\u001b[38;5;241m.\u001b[39mcontains(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_word):\n",
      "File \u001b[0;32m~/miniconda3/envs/bismillah/lib/python3.10/site-packages/Sastrawi/Stemmer/Context/Context.py:80\u001b[0m, in \u001b[0;36mContext.start_stemming_process\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m#step 4, 5\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremove_prefixes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdictionary\u001b[38;5;241m.\u001b[39mcontains(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_word):\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bismillah/lib/python3.10/site-packages/Sastrawi/Stemmer/Context/Context.py:89\u001b[0m, in \u001b[0;36mContext.remove_prefixes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mremove_prefixes\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m---> 89\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccept_prefix_visitors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprefix_pisitors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdictionary\u001b[38;5;241m.\u001b[39mcontains(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_word):\n\u001b[1;32m     91\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bismillah/lib/python3.10/site-packages/Sastrawi/Stemmer/Context/Context.py:110\u001b[0m, in \u001b[0;36mContext.accept_prefix_visitors\u001b[0;34m(self, visitors)\u001b[0m\n\u001b[1;32m    108\u001b[0m removalCount \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremovals)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m visitor \u001b[38;5;129;01min\u001b[39;00m visitors:\n\u001b[0;32m--> 110\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccept\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvisitor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdictionary\u001b[38;5;241m.\u001b[39mcontains(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_word):\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_word\n",
      "File \u001b[0;32m~/miniconda3/envs/bismillah/lib/python3.10/site-packages/Sastrawi/Stemmer/Context/Context.py:97\u001b[0m, in \u001b[0;36mContext.accept\u001b[0;34m(self, visitor)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maccept\u001b[39m(\u001b[38;5;28mself\u001b[39m, visitor):\n\u001b[0;32m---> 97\u001b[0m     \u001b[43mvisitor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bismillah/lib/python3.10/site-packages/Sastrawi/Stemmer/Context/Visitor/AbstractDisambiguatePrefixRule.py:15\u001b[0m, in \u001b[0;36mAbstractDisambiguatePrefixRule.visit\u001b[0;34m(self, context)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m disambiguator \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisambiguators:\n\u001b[1;32m     14\u001b[0m     result \u001b[38;5;241m=\u001b[39m disambiguator\u001b[38;5;241m.\u001b[39mdisambiguate(context\u001b[38;5;241m.\u001b[39mcurrent_word)\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdictionary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontains\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# df_anis['stemmed'] = tokenize_anis.apply(stemming)\n",
    "# df_prabowo['stemmed'] = tokenize_prabowo.apply(stemming)\n",
    "# df_ganjar['stemmed'] = tokenize_ganjar.apply(stemming)\n",
    "\n",
    "\n",
    "# tokenize_anis = tokenize_anis.apply(stemming,token=True)\n",
    "# tokenize_prabowo = tokenize_prabowo.apply(stemming,token=True)\n",
    "# tokenize_ganjar = tokenize_ganjar.apply(stemming,token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword = StopWordRemoverFactory().create_stop_word_remover().dictionary.words\n",
    "\n",
    "def clear_stopwords(data,token=False):\n",
    "    temp = []\n",
    "    last = ''\n",
    "    for tup in data:\n",
    "        if(tup[0] in stopword):\n",
    "            continue\n",
    "        if token:\n",
    "            temp.append(tup)\n",
    "            last = temp\n",
    "        else:\n",
    "            temp.append(tup[0])\n",
    "            last = ' '.join(temp)\n",
    "    return last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anis['no_stopwords'] = tokenize_anis.apply(clear_stopwords)\n",
    "df_prabowo['no_stopwords'] = tokenize_prabowo.apply(clear_stopwords)\n",
    "df_ganjar['no_stopwords'] = tokenize_ganjar.apply(clear_stopwords)\n",
    "\n",
    "\n",
    "tokenize_anis = tokenize_anis.apply(clear_stopwords,token=True)\n",
    "tokenize_prabowo = tokenize_prabowo.apply(clear_stopwords,token=True)\n",
    "tokenize_ganjar = tokenize_ganjar.apply(clear_stopwords,token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join(data):\n",
    "    temp = []\n",
    "    for tup in data:\n",
    "        temp.append(tup[1])\n",
    "    return' '.join(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anis['tag'] = tokenize_anis.apply(join)\n",
    "df_prabowo['tag'] = tokenize_prabowo.apply(join)\n",
    "df_ganjar['tag'] = tokenize_ganjar.apply(join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removena(data):\n",
    "    if len(data) == 0:\n",
    "        return None\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anis['no_stopwords'] = df_anis['no_stopwords'].apply(removena)\n",
    "df_prabowo['no_stopwords'] = df_prabowo['no_stopwords'].apply(removena)\n",
    "df_ganjar['no_stopwords'] = df_ganjar['no_stopwords'].apply(removena)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tweet            0\n",
       "label            0\n",
       "no_double        0\n",
       "no_alay          0\n",
       "pos              0\n",
       "clean_manual     0\n",
       "stemmed          0\n",
       "no_stopwords    90\n",
       "tag              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_anis.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8913, 9)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_anis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anis.dropna(inplace=True)\n",
    "df_prabowo.dropna(inplace=True)\n",
    "df_ganjar.dropna(inplace=True)\n",
    "\n",
    "df_anis.reset_index(drop=True, inplace=True)\n",
    "df_prabowo.reset_index(drop=True, inplace=True)\n",
    "df_ganjar.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8823, 9)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_anis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tweet           0\n",
       "label           0\n",
       "no_double       0\n",
       "no_alay         0\n",
       "pos             0\n",
       "clean_manual    0\n",
       "stemmed         0\n",
       "no_stopwords    0\n",
       "tag             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_anis.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anis = df_anis[['Tweet','no_double','no_alay','pos','clean_manual','stemmed','no_stopwords','tag','label']]\n",
    "df_prabowo = df_prabowo[['Tweet','no_double','no_alay','pos','clean_manual','stemmed','no_stopwords','tag','label']]\n",
    "df_ganjar = df_ganjar[['Tweet','no_double','no_alay','pos','clean_manual','stemmed','no_stopwords','tag','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat = pd.concat([df_anis,df_prabowo,df_ganjar])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(dtype=int)\n",
    "pos_vec = vectorizer.fit(concat['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vectorizer, open(\"pickle/countVectorizer_tag.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = vectorizer.transform(df_anis['tag'])\n",
    "pos_tag = pd.DataFrame(temp.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "df_anis = pd.concat([df_anis, pos_tag], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = vectorizer.transform(df_prabowo['tag'])\n",
    "pos_tag = pd.DataFrame(temp.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "df_prabowo = pd.concat([df_prabowo, pos_tag], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = vectorizer.transform(df_ganjar['tag'])\n",
    "pos_tag = pd.DataFrame(temp.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "df_ganjar = pd.concat([df_ganjar, pos_tag], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat = pd.concat([df_anis,df_prabowo,df_ganjar])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = os.path.abspath(os.path.join('..', 'data/clean/pos_tagging'))\n",
    "df_anis.to_csv(f'{dir}/anis.csv',index=False)\n",
    "df_prabowo.to_csv(f'{dir}/prabowo.csv',index=False)\n",
    "df_ganjar.to_csv(f'{dir}/ganjar.csv',index=False)\n",
    "concat.to_csv(f'{dir}/gabungan.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bismillah",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
