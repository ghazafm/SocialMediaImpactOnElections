{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fauzanghaza/Applications/miniconda3/envs/ML/lib/python3.9/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/Users/fauzanghaza/Applications/miniconda3/envs/ML/lib/python3.9/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
      "  Referenced from: <FB2FD416-6C4D-3621-B677-61F07C02A3C5> /Users/fauzanghaza/Applications/miniconda3/envs/ML/lib/python3.9/site-packages/torchvision/image.so\n",
      "  Reason: tried: '/Users/fauzanghaza/Applications/miniconda3/envs/ML/lib/python3.9/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/fauzanghaza/Applications/miniconda3/envs/ML/lib/python3.9/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/fauzanghaza/Applications/miniconda3/envs/ML/lib/python3.9/lib-dynload/../../libjpeg.9.dylib' (no such file), '/Users/fauzanghaza/Applications/miniconda3/envs/ML/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import Funct_modelling\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameter\n",
    "input_size = 15474\n",
    "hidden_size = 100\n",
    "num_epochs = 100\n",
    "batch_size = 128\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess & vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElectionDataset(Dataset):\n",
    "    def __init__(self, features, labels, train=True, transform=None, test_size=0.2, random_state=42):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features (numpy array): The feature array of shape (num_samples, num_features)\n",
    "            labels (numpy array): The label array of shape (num_samples,)\n",
    "            train (bool): If True, loads training data, if False loads test data\n",
    "            transform (callable, optional): Optional transform to be applied on a sample\n",
    "            test_size (float): Proportion of the dataset to include in the test split\n",
    "            random_state (int): Seed for reproducibility of the split\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "        X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=test_size, random_state=random_state)\n",
    "\n",
    "        if self.train:\n",
    "            self.x = X_train\n",
    "            self.y = y_train\n",
    "        else:\n",
    "            self.x = X_test\n",
    "            self.y = y_test\n",
    "\n",
    "        self.n_samples = len(self.x)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if index >= self.n_samples:\n",
    "            raise IndexError(\"Index out of range\")\n",
    "\n",
    "        sample = self.x[index], self.y[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor:\n",
    "    def __call__(self, sample):\n",
    "        inputs, label = sample\n",
    "        return torch.tensor(inputs, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)\n",
    "    \n",
    "class DropNA:\n",
    "    def __call__(self, sample):\n",
    "        inputs, label = sample\n",
    "        inputs = pd.DataFrame(inputs)\n",
    "        inputs = inputs.dropna(axis=0, how='any') \n",
    "        label = label[inputs.index]\n",
    "        print(label)\n",
    "        print(inputs.index)\n",
    "        \n",
    "        return inputs.values, label\n",
    "    \n",
    "class Preprocessing:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def undersampling(self):\n",
    "        x_p = self.df[self.df['label'] == 'Positive']\n",
    "        x_n = self.df[self.df['label'] == 'Negative']\n",
    "        x_temp = x_p.sample(x_n.label.count(), random_state=42)\n",
    "        x_under = pd.concat([x_temp, x_n], axis=0, ignore_index=True)\n",
    "        return x_under\n",
    "\n",
    "    def tfidf_vec(self, data):\n",
    "        # Transform the text data using TF-IDF\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        data = vectorizer.fit_transform(data)\n",
    "        data = data.toarray()\n",
    "        data = pd.DataFrame(data, columns=vectorizer.get_feature_names_out())\n",
    "        return data\n",
    "\n",
    "\n",
    "    def process(self, tag=False):\n",
    "        self.df = self.undersampling()\n",
    "        tag_data = []\n",
    "\n",
    "        if tag:\n",
    "            tag_data = self.df[self.df.columns[-20:]]\n",
    "\n",
    "        X = self.df['no_stopwords']\n",
    "        y = self.df['label']\n",
    "\n",
    "        # Convert the text data using TF-IDF\n",
    "        X = self.tfidf_vec(X)\n",
    "\n",
    "        # Optionally concatenate tag data if required\n",
    "        if tag:\n",
    "            tag_data = tag_data.values  # Convert tag data to numpy if present\n",
    "            X = np.hstack((X, tag_data))\n",
    "        \n",
    "        y = y.values\n",
    "\n",
    "        # Train-test split\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../preprocessing/Training/data/clean/regular/gabungan.csv')\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocessing(df)\n",
    "X, y = preprocessor.process(tag=False)\n",
    "X = X.to_numpy()\n",
    "label_mapping = {\n",
    "    'Positive': 1,\n",
    "    'Negative': 0\n",
    "}\n",
    "# Apply the mapping to your labels\n",
    "y = np.array([label_mapping[label] for label in y])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = torchvision.transforms.Compose([ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'X' and 'y' are your features and labels\n",
    "train_dataset = ElectionDataset(features=X, labels=y, train=True, transform=transformer, random_state=42)\n",
    "test_dataset = ElectionDataset(features=X, labels=y, train=False, transform=transformer, random_state=42)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([128, 15474]) torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "examples = iter(train_loader)\n",
    "example_data, example_targets = next(examples)\n",
    "print(example_data)\n",
    "print(example_data.shape, example_targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        self.l1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.l2 = nn.Linear(hidden_size, hidden_size * 2) \n",
    "        self.relu2 = nn.LeakyReLU(negative_slope=0.01)\n",
    "        \n",
    "        self.l3 = nn.Linear(hidden_size * 2, hidden_size * 2)\n",
    "        self.relu3 = nn.LeakyReLU(negative_slope=0.01)\n",
    "        \n",
    "        self.l4 = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        \n",
    "        self.l5 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu5 = nn.LeakyReLU(negative_slope=0.01)\n",
    "        \n",
    "        self.l6 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.relu6 = nn.ReLU()\n",
    "\n",
    "        self.l7 = nn.Linear(hidden_size // 2, 1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(hidden_size * 2)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # First layer\n",
    "        out = self.l1(x)\n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        # Second layer (with BatchNorm and Dropout)\n",
    "        out = self.l2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.batch_norm1(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Third layer\n",
    "        out = self.l3(out)\n",
    "        out = self.relu3(out)\n",
    "        out = self.batch_norm1(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Fourth layer\n",
    "        out = self.l4(out)\n",
    "        out = self.relu4(out)\n",
    "        \n",
    "        # Fifth layer\n",
    "        out = self.l5(out)\n",
    "        out = self.relu5(out)\n",
    "        \n",
    "        # Sixth layer (bottleneck)\n",
    "        out = self.l6(out)\n",
    "        out = self.relu6(out)\n",
    "        \n",
    "        # Final output layer\n",
    "        out = self.l7(out)\n",
    "        out = torch.sigmoid(out)  # Sigmoid for binary classification\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(input_size, hidden_size).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/100, step 22/110, loss = 0.7045\n",
      "epoch 1/100, step 44/110, loss = 0.6908\n",
      "epoch 1/100, step 66/110, loss = 0.6994\n",
      "epoch 1/100, step 88/110, loss = 0.6943\n",
      "epoch 1/100, step 110/110, loss = 0.6961\n",
      "epoch 2/100, step 22/110, loss = 0.6947\n",
      "epoch 2/100, step 44/110, loss = 0.6895\n",
      "epoch 2/100, step 66/110, loss = 0.6967\n",
      "epoch 2/100, step 88/110, loss = 0.7000\n",
      "epoch 2/100, step 110/110, loss = 0.7032\n",
      "epoch 3/100, step 22/110, loss = 0.6926\n",
      "epoch 3/100, step 44/110, loss = 0.6960\n",
      "epoch 3/100, step 66/110, loss = 0.7014\n",
      "epoch 3/100, step 88/110, loss = 0.6933\n",
      "epoch 3/100, step 110/110, loss = 0.6751\n",
      "epoch 4/100, step 22/110, loss = 0.6908\n",
      "epoch 4/100, step 44/110, loss = 0.6924\n",
      "epoch 4/100, step 66/110, loss = 0.6999\n",
      "epoch 4/100, step 88/110, loss = 0.6877\n",
      "epoch 4/100, step 110/110, loss = 0.6914\n",
      "epoch 5/100, step 22/110, loss = 0.6890\n",
      "epoch 5/100, step 44/110, loss = 0.7025\n",
      "epoch 5/100, step 66/110, loss = 0.6951\n",
      "epoch 5/100, step 88/110, loss = 0.6921\n",
      "epoch 5/100, step 110/110, loss = 0.6823\n",
      "epoch 6/100, step 22/110, loss = 0.7015\n",
      "epoch 6/100, step 44/110, loss = 0.6911\n",
      "epoch 6/100, step 66/110, loss = 0.6938\n",
      "epoch 6/100, step 88/110, loss = 0.6899\n",
      "epoch 6/100, step 110/110, loss = 0.6888\n",
      "epoch 7/100, step 22/110, loss = 0.6897\n",
      "epoch 7/100, step 44/110, loss = 0.6882\n",
      "epoch 7/100, step 66/110, loss = 0.6982\n",
      "epoch 7/100, step 88/110, loss = 0.6871\n",
      "epoch 7/100, step 110/110, loss = 0.7009\n",
      "epoch 8/100, step 22/110, loss = 0.6910\n",
      "epoch 8/100, step 44/110, loss = 0.6918\n",
      "epoch 8/100, step 66/110, loss = 0.6907\n",
      "epoch 8/100, step 88/110, loss = 0.6959\n",
      "epoch 8/100, step 110/110, loss = 0.6992\n",
      "epoch 9/100, step 22/110, loss = 0.6922\n",
      "epoch 9/100, step 44/110, loss = 0.6947\n",
      "epoch 9/100, step 66/110, loss = 0.6943\n",
      "epoch 9/100, step 88/110, loss = 0.6939\n",
      "epoch 9/100, step 110/110, loss = 0.6847\n",
      "epoch 10/100, step 22/110, loss = 0.6908\n",
      "epoch 10/100, step 44/110, loss = 0.6926\n",
      "epoch 10/100, step 66/110, loss = 0.6947\n",
      "epoch 10/100, step 88/110, loss = 0.6889\n",
      "epoch 10/100, step 110/110, loss = 0.6928\n",
      "epoch 11/100, step 22/110, loss = 0.6998\n",
      "epoch 11/100, step 44/110, loss = 0.6900\n",
      "epoch 11/100, step 66/110, loss = 0.6958\n",
      "epoch 11/100, step 88/110, loss = 0.6963\n",
      "epoch 11/100, step 110/110, loss = 0.6892\n",
      "epoch 12/100, step 22/110, loss = 0.6972\n",
      "epoch 12/100, step 44/110, loss = 0.6967\n",
      "epoch 12/100, step 66/110, loss = 0.6943\n",
      "epoch 12/100, step 88/110, loss = 0.6887\n",
      "epoch 12/100, step 110/110, loss = 0.6932\n",
      "epoch 13/100, step 22/110, loss = 0.6976\n",
      "epoch 13/100, step 44/110, loss = 0.6977\n",
      "epoch 13/100, step 66/110, loss = 0.6917\n",
      "epoch 13/100, step 88/110, loss = 0.6917\n",
      "epoch 13/100, step 110/110, loss = 0.6936\n",
      "epoch 14/100, step 22/110, loss = 0.6903\n",
      "epoch 14/100, step 44/110, loss = 0.6911\n",
      "epoch 14/100, step 66/110, loss = 0.6959\n",
      "epoch 14/100, step 88/110, loss = 0.6940\n",
      "epoch 14/100, step 110/110, loss = 0.7000\n",
      "epoch 15/100, step 22/110, loss = 0.6947\n",
      "epoch 15/100, step 44/110, loss = 0.6929\n",
      "epoch 15/100, step 66/110, loss = 0.6901\n",
      "epoch 15/100, step 88/110, loss = 0.6915\n",
      "epoch 15/100, step 110/110, loss = 0.6909\n",
      "epoch 16/100, step 22/110, loss = 0.6876\n",
      "epoch 16/100, step 44/110, loss = 0.6961\n",
      "epoch 16/100, step 66/110, loss = 0.6880\n",
      "epoch 16/100, step 88/110, loss = 0.6909\n",
      "epoch 16/100, step 110/110, loss = 0.6869\n",
      "epoch 17/100, step 22/110, loss = 0.6846\n",
      "epoch 17/100, step 44/110, loss = 0.6939\n",
      "epoch 17/100, step 66/110, loss = 0.6928\n",
      "epoch 17/100, step 88/110, loss = 0.6945\n",
      "epoch 17/100, step 110/110, loss = 0.6908\n",
      "epoch 18/100, step 22/110, loss = 0.6920\n",
      "epoch 18/100, step 44/110, loss = 0.6950\n",
      "epoch 18/100, step 66/110, loss = 0.6908\n",
      "epoch 18/100, step 88/110, loss = 0.6937\n",
      "epoch 18/100, step 110/110, loss = 0.6928\n",
      "epoch 19/100, step 22/110, loss = 0.6917\n",
      "epoch 19/100, step 44/110, loss = 0.6929\n",
      "epoch 19/100, step 66/110, loss = 0.6901\n",
      "epoch 19/100, step 88/110, loss = 0.6889\n",
      "epoch 19/100, step 110/110, loss = 0.6986\n",
      "epoch 20/100, step 22/110, loss = 0.6924\n",
      "epoch 20/100, step 44/110, loss = 0.6911\n",
      "epoch 20/100, step 66/110, loss = 0.6910\n",
      "epoch 20/100, step 88/110, loss = 0.6929\n",
      "epoch 20/100, step 110/110, loss = 0.6868\n",
      "epoch 21/100, step 22/110, loss = 0.6935\n",
      "epoch 21/100, step 44/110, loss = 0.6917\n",
      "epoch 21/100, step 66/110, loss = 0.6945\n",
      "epoch 21/100, step 88/110, loss = 0.6912\n",
      "epoch 21/100, step 110/110, loss = 0.6934\n",
      "epoch 22/100, step 22/110, loss = 0.6924\n",
      "epoch 22/100, step 44/110, loss = 0.6967\n",
      "epoch 22/100, step 66/110, loss = 0.6943\n",
      "epoch 22/100, step 88/110, loss = 0.6914\n",
      "epoch 22/100, step 110/110, loss = 0.6932\n",
      "epoch 23/100, step 22/110, loss = 0.6912\n",
      "epoch 23/100, step 44/110, loss = 0.6915\n",
      "epoch 23/100, step 66/110, loss = 0.6962\n",
      "epoch 23/100, step 88/110, loss = 0.6892\n",
      "epoch 23/100, step 110/110, loss = 0.6913\n",
      "epoch 24/100, step 22/110, loss = 0.6951\n",
      "epoch 24/100, step 44/110, loss = 0.6945\n",
      "epoch 24/100, step 66/110, loss = 0.6919\n",
      "epoch 24/100, step 88/110, loss = 0.6922\n",
      "epoch 24/100, step 110/110, loss = 0.6970\n",
      "epoch 25/100, step 22/110, loss = 0.6898\n",
      "epoch 25/100, step 44/110, loss = 0.6904\n",
      "epoch 25/100, step 66/110, loss = 0.6925\n",
      "epoch 25/100, step 88/110, loss = 0.6890\n",
      "epoch 25/100, step 110/110, loss = 0.6949\n",
      "epoch 26/100, step 22/110, loss = 0.6931\n",
      "epoch 26/100, step 44/110, loss = 0.6952\n",
      "epoch 26/100, step 66/110, loss = 0.6918\n",
      "epoch 26/100, step 88/110, loss = 0.6955\n",
      "epoch 26/100, step 110/110, loss = 0.6944\n",
      "epoch 27/100, step 22/110, loss = 0.6894\n",
      "epoch 27/100, step 44/110, loss = 0.6937\n",
      "epoch 27/100, step 66/110, loss = 0.6918\n",
      "epoch 27/100, step 88/110, loss = 0.6947\n",
      "epoch 27/100, step 110/110, loss = 0.6913\n",
      "epoch 28/100, step 22/110, loss = 0.6920\n",
      "epoch 28/100, step 44/110, loss = 0.6934\n",
      "epoch 28/100, step 66/110, loss = 0.6954\n",
      "epoch 28/100, step 88/110, loss = 0.6980\n",
      "epoch 28/100, step 110/110, loss = 0.6904\n",
      "epoch 29/100, step 22/110, loss = 0.6910\n",
      "epoch 29/100, step 44/110, loss = 0.6936\n",
      "epoch 29/100, step 66/110, loss = 0.6899\n",
      "epoch 29/100, step 88/110, loss = 0.6911\n",
      "epoch 29/100, step 110/110, loss = 0.6941\n",
      "epoch 30/100, step 22/110, loss = 0.6894\n",
      "epoch 30/100, step 44/110, loss = 0.6920\n",
      "epoch 30/100, step 66/110, loss = 0.6893\n",
      "epoch 30/100, step 88/110, loss = 0.6889\n",
      "epoch 30/100, step 110/110, loss = 0.6921\n",
      "epoch 31/100, step 22/110, loss = 0.6946\n",
      "epoch 31/100, step 44/110, loss = 0.6926\n",
      "epoch 31/100, step 66/110, loss = 0.6960\n",
      "epoch 31/100, step 88/110, loss = 0.6945\n",
      "epoch 31/100, step 110/110, loss = 0.6972\n",
      "epoch 32/100, step 22/110, loss = 0.6905\n",
      "epoch 32/100, step 44/110, loss = 0.6935\n",
      "epoch 32/100, step 66/110, loss = 0.6968\n",
      "epoch 32/100, step 88/110, loss = 0.6889\n",
      "epoch 32/100, step 110/110, loss = 0.6936\n",
      "epoch 33/100, step 22/110, loss = 0.6909\n",
      "epoch 33/100, step 44/110, loss = 0.6902\n",
      "epoch 33/100, step 66/110, loss = 0.6919\n",
      "epoch 33/100, step 88/110, loss = 0.6930\n",
      "epoch 33/100, step 110/110, loss = 0.6896\n",
      "epoch 34/100, step 22/110, loss = 0.6921\n",
      "epoch 34/100, step 44/110, loss = 0.6905\n",
      "epoch 34/100, step 66/110, loss = 0.6907\n",
      "epoch 34/100, step 88/110, loss = 0.6940\n",
      "epoch 34/100, step 110/110, loss = 0.6888\n",
      "epoch 35/100, step 22/110, loss = 0.6885\n",
      "epoch 35/100, step 44/110, loss = 0.6879\n",
      "epoch 35/100, step 66/110, loss = 0.6927\n",
      "epoch 35/100, step 88/110, loss = 0.6904\n",
      "epoch 35/100, step 110/110, loss = 0.6935\n",
      "epoch 36/100, step 22/110, loss = 0.6928\n",
      "epoch 36/100, step 44/110, loss = 0.6886\n",
      "epoch 36/100, step 66/110, loss = 0.6936\n",
      "epoch 36/100, step 88/110, loss = 0.6932\n",
      "epoch 36/100, step 110/110, loss = 0.6916\n",
      "epoch 37/100, step 22/110, loss = 0.6923\n",
      "epoch 37/100, step 44/110, loss = 0.6932\n",
      "epoch 37/100, step 66/110, loss = 0.6940\n",
      "epoch 37/100, step 88/110, loss = 0.6921\n",
      "epoch 37/100, step 110/110, loss = 0.6935\n",
      "epoch 38/100, step 22/110, loss = 0.6918\n",
      "epoch 38/100, step 44/110, loss = 0.6912\n",
      "epoch 38/100, step 66/110, loss = 0.6875\n",
      "epoch 38/100, step 88/110, loss = 0.6962\n",
      "epoch 38/100, step 110/110, loss = 0.6897\n",
      "epoch 39/100, step 22/110, loss = 0.6929\n",
      "epoch 39/100, step 44/110, loss = 0.6897\n",
      "epoch 39/100, step 66/110, loss = 0.6895\n",
      "epoch 39/100, step 88/110, loss = 0.6928\n",
      "epoch 39/100, step 110/110, loss = 0.6909\n",
      "epoch 40/100, step 22/110, loss = 0.6919\n",
      "epoch 40/100, step 44/110, loss = 0.6947\n",
      "epoch 40/100, step 66/110, loss = 0.6907\n",
      "epoch 40/100, step 88/110, loss = 0.6927\n",
      "epoch 40/100, step 110/110, loss = 0.6953\n",
      "epoch 41/100, step 22/110, loss = 0.6913\n",
      "epoch 41/100, step 44/110, loss = 0.6889\n",
      "epoch 41/100, step 66/110, loss = 0.6919\n",
      "epoch 41/100, step 88/110, loss = 0.6908\n",
      "epoch 41/100, step 110/110, loss = 0.6913\n",
      "epoch 42/100, step 22/110, loss = 0.6903\n",
      "epoch 42/100, step 44/110, loss = 0.6909\n",
      "epoch 42/100, step 66/110, loss = 0.6936\n",
      "epoch 42/100, step 88/110, loss = 0.6904\n",
      "epoch 42/100, step 110/110, loss = 0.6910\n",
      "epoch 43/100, step 22/110, loss = 0.6911\n",
      "epoch 43/100, step 44/110, loss = 0.6906\n",
      "epoch 43/100, step 66/110, loss = 0.6923\n",
      "epoch 43/100, step 88/110, loss = 0.6922\n",
      "epoch 43/100, step 110/110, loss = 0.6906\n",
      "epoch 44/100, step 22/110, loss = 0.6916\n",
      "epoch 44/100, step 44/110, loss = 0.6890\n",
      "epoch 44/100, step 66/110, loss = 0.6904\n",
      "epoch 44/100, step 88/110, loss = 0.6922\n",
      "epoch 44/100, step 110/110, loss = 0.6956\n",
      "epoch 45/100, step 22/110, loss = 0.6935\n",
      "epoch 45/100, step 44/110, loss = 0.6917\n",
      "epoch 45/100, step 66/110, loss = 0.6943\n",
      "epoch 45/100, step 88/110, loss = 0.6928\n",
      "epoch 45/100, step 110/110, loss = 0.6952\n",
      "epoch 46/100, step 22/110, loss = 0.6895\n",
      "epoch 46/100, step 44/110, loss = 0.6919\n",
      "epoch 46/100, step 66/110, loss = 0.6907\n",
      "epoch 46/100, step 88/110, loss = 0.6920\n",
      "epoch 46/100, step 110/110, loss = 0.6915\n",
      "epoch 47/100, step 22/110, loss = 0.6911\n",
      "epoch 47/100, step 44/110, loss = 0.6909\n",
      "epoch 47/100, step 66/110, loss = 0.6902\n",
      "epoch 47/100, step 88/110, loss = 0.6906\n",
      "epoch 47/100, step 110/110, loss = 0.6904\n",
      "epoch 48/100, step 22/110, loss = 0.6950\n",
      "epoch 48/100, step 44/110, loss = 0.6934\n",
      "epoch 48/100, step 66/110, loss = 0.6930\n",
      "epoch 48/100, step 88/110, loss = 0.6909\n",
      "epoch 48/100, step 110/110, loss = 0.6910\n",
      "epoch 49/100, step 22/110, loss = 0.6898\n",
      "epoch 49/100, step 44/110, loss = 0.6932\n",
      "epoch 49/100, step 66/110, loss = 0.6907\n",
      "epoch 49/100, step 88/110, loss = 0.6942\n",
      "epoch 49/100, step 110/110, loss = 0.6922\n",
      "epoch 50/100, step 22/110, loss = 0.6905\n",
      "epoch 50/100, step 44/110, loss = 0.6888\n",
      "epoch 50/100, step 66/110, loss = 0.6896\n",
      "epoch 50/100, step 88/110, loss = 0.6894\n",
      "epoch 50/100, step 110/110, loss = 0.6925\n",
      "epoch 51/100, step 22/110, loss = 0.6931\n",
      "epoch 51/100, step 44/110, loss = 0.6916\n",
      "epoch 51/100, step 66/110, loss = 0.6914\n",
      "epoch 51/100, step 88/110, loss = 0.6921\n",
      "epoch 51/100, step 110/110, loss = 0.6926\n",
      "epoch 52/100, step 22/110, loss = 0.6915\n",
      "epoch 52/100, step 44/110, loss = 0.6901\n",
      "epoch 52/100, step 66/110, loss = 0.6921\n",
      "epoch 52/100, step 88/110, loss = 0.6928\n",
      "epoch 52/100, step 110/110, loss = 0.6937\n",
      "epoch 53/100, step 22/110, loss = 0.6917\n",
      "epoch 53/100, step 44/110, loss = 0.6903\n",
      "epoch 53/100, step 66/110, loss = 0.6899\n",
      "epoch 53/100, step 88/110, loss = 0.6910\n",
      "epoch 53/100, step 110/110, loss = 0.6875\n",
      "epoch 54/100, step 22/110, loss = 0.6882\n",
      "epoch 54/100, step 44/110, loss = 0.6895\n",
      "epoch 54/100, step 66/110, loss = 0.6912\n",
      "epoch 54/100, step 88/110, loss = 0.6931\n",
      "epoch 54/100, step 110/110, loss = 0.6871\n",
      "epoch 55/100, step 22/110, loss = 0.6901\n",
      "epoch 55/100, step 44/110, loss = 0.6907\n",
      "epoch 55/100, step 66/110, loss = 0.6908\n",
      "epoch 55/100, step 88/110, loss = 0.6898\n",
      "epoch 55/100, step 110/110, loss = 0.6916\n",
      "epoch 56/100, step 22/110, loss = 0.6884\n",
      "epoch 56/100, step 44/110, loss = 0.6900\n",
      "epoch 56/100, step 66/110, loss = 0.6891\n",
      "epoch 56/100, step 88/110, loss = 0.6906\n",
      "epoch 56/100, step 110/110, loss = 0.6875\n",
      "epoch 57/100, step 22/110, loss = 0.6888\n",
      "epoch 57/100, step 44/110, loss = 0.6877\n",
      "epoch 57/100, step 66/110, loss = 0.6863\n",
      "epoch 57/100, step 88/110, loss = 0.6924\n",
      "epoch 57/100, step 110/110, loss = 0.6898\n",
      "epoch 58/100, step 22/110, loss = 0.6922\n",
      "epoch 58/100, step 44/110, loss = 0.6872\n",
      "epoch 58/100, step 66/110, loss = 0.6888\n",
      "epoch 58/100, step 88/110, loss = 0.6880\n",
      "epoch 58/100, step 110/110, loss = 0.6914\n",
      "epoch 59/100, step 22/110, loss = 0.6870\n",
      "epoch 59/100, step 44/110, loss = 0.6872\n",
      "epoch 59/100, step 66/110, loss = 0.6908\n",
      "epoch 59/100, step 88/110, loss = 0.6883\n",
      "epoch 59/100, step 110/110, loss = 0.6912\n",
      "epoch 60/100, step 22/110, loss = 0.6878\n",
      "epoch 60/100, step 44/110, loss = 0.6924\n",
      "epoch 60/100, step 66/110, loss = 0.6897\n",
      "epoch 60/100, step 88/110, loss = 0.6898\n",
      "epoch 60/100, step 110/110, loss = 0.6871\n",
      "epoch 61/100, step 22/110, loss = 0.6869\n",
      "epoch 61/100, step 44/110, loss = 0.6882\n",
      "epoch 61/100, step 66/110, loss = 0.6895\n",
      "epoch 61/100, step 88/110, loss = 0.6899\n",
      "epoch 61/100, step 110/110, loss = 0.6892\n",
      "epoch 62/100, step 22/110, loss = 0.6917\n",
      "epoch 62/100, step 44/110, loss = 0.6895\n",
      "epoch 62/100, step 66/110, loss = 0.6856\n",
      "epoch 62/100, step 88/110, loss = 0.6928\n",
      "epoch 62/100, step 110/110, loss = 0.6897\n",
      "epoch 63/100, step 22/110, loss = 0.6921\n",
      "epoch 63/100, step 44/110, loss = 0.6869\n",
      "epoch 63/100, step 66/110, loss = 0.6891\n",
      "epoch 63/100, step 88/110, loss = 0.6908\n",
      "epoch 63/100, step 110/110, loss = 0.6870\n",
      "epoch 64/100, step 22/110, loss = 0.6911\n",
      "epoch 64/100, step 44/110, loss = 0.6932\n",
      "epoch 64/100, step 66/110, loss = 0.6896\n",
      "epoch 64/100, step 88/110, loss = 0.6878\n",
      "epoch 64/100, step 110/110, loss = 0.6938\n",
      "epoch 65/100, step 22/110, loss = 0.6895\n",
      "epoch 65/100, step 44/110, loss = 0.6905\n",
      "epoch 65/100, step 66/110, loss = 0.6912\n",
      "epoch 65/100, step 88/110, loss = 0.6869\n",
      "epoch 65/100, step 110/110, loss = 0.6912\n",
      "epoch 66/100, step 22/110, loss = 0.6950\n",
      "epoch 66/100, step 44/110, loss = 0.6861\n",
      "epoch 66/100, step 66/110, loss = 0.6897\n",
      "epoch 66/100, step 88/110, loss = 0.6901\n",
      "epoch 66/100, step 110/110, loss = 0.6909\n",
      "epoch 67/100, step 22/110, loss = 0.6877\n",
      "epoch 67/100, step 44/110, loss = 0.6879\n",
      "epoch 67/100, step 66/110, loss = 0.6865\n",
      "epoch 67/100, step 88/110, loss = 0.6894\n",
      "epoch 67/100, step 110/110, loss = 0.6926\n",
      "epoch 68/100, step 22/110, loss = 0.6860\n",
      "epoch 68/100, step 44/110, loss = 0.6862\n",
      "epoch 68/100, step 66/110, loss = 0.6832\n",
      "epoch 68/100, step 88/110, loss = 0.6908\n",
      "epoch 68/100, step 110/110, loss = 0.6899\n",
      "epoch 69/100, step 22/110, loss = 0.6906\n",
      "epoch 69/100, step 44/110, loss = 0.6872\n",
      "epoch 69/100, step 66/110, loss = 0.6851\n",
      "epoch 69/100, step 88/110, loss = 0.6910\n",
      "epoch 69/100, step 110/110, loss = 0.6897\n",
      "epoch 70/100, step 22/110, loss = 0.6866\n",
      "epoch 70/100, step 44/110, loss = 0.6910\n",
      "epoch 70/100, step 66/110, loss = 0.6878\n",
      "epoch 70/100, step 88/110, loss = 0.6881\n",
      "epoch 70/100, step 110/110, loss = 0.6891\n",
      "epoch 71/100, step 22/110, loss = 0.6916\n",
      "epoch 71/100, step 44/110, loss = 0.6918\n",
      "epoch 71/100, step 66/110, loss = 0.6868\n",
      "epoch 71/100, step 88/110, loss = 0.6879\n",
      "epoch 71/100, step 110/110, loss = 0.6901\n",
      "epoch 72/100, step 22/110, loss = 0.6859\n",
      "epoch 72/100, step 44/110, loss = 0.6868\n",
      "epoch 72/100, step 66/110, loss = 0.6894\n",
      "epoch 72/100, step 88/110, loss = 0.6871\n",
      "epoch 72/100, step 110/110, loss = 0.6916\n",
      "epoch 73/100, step 22/110, loss = 0.6879\n",
      "epoch 73/100, step 44/110, loss = 0.6906\n",
      "epoch 73/100, step 66/110, loss = 0.6855\n",
      "epoch 73/100, step 88/110, loss = 0.6855\n",
      "epoch 73/100, step 110/110, loss = 0.6853\n",
      "epoch 74/100, step 22/110, loss = 0.6844\n",
      "epoch 74/100, step 44/110, loss = 0.6867\n",
      "epoch 74/100, step 66/110, loss = 0.6869\n",
      "epoch 74/100, step 88/110, loss = 0.6894\n",
      "epoch 74/100, step 110/110, loss = 0.6874\n",
      "epoch 75/100, step 22/110, loss = 0.6899\n",
      "epoch 75/100, step 44/110, loss = 0.6819\n",
      "epoch 75/100, step 66/110, loss = 0.6834\n",
      "epoch 75/100, step 88/110, loss = 0.6883\n",
      "epoch 75/100, step 110/110, loss = 0.6870\n",
      "epoch 76/100, step 22/110, loss = 0.6883\n",
      "epoch 76/100, step 44/110, loss = 0.6893\n",
      "epoch 76/100, step 66/110, loss = 0.6826\n",
      "epoch 76/100, step 88/110, loss = 0.6856\n",
      "epoch 76/100, step 110/110, loss = 0.6830\n",
      "epoch 77/100, step 22/110, loss = 0.6873\n",
      "epoch 77/100, step 44/110, loss = 0.6906\n",
      "epoch 77/100, step 66/110, loss = 0.6863\n",
      "epoch 77/100, step 88/110, loss = 0.6849\n",
      "epoch 77/100, step 110/110, loss = 0.6828\n",
      "epoch 78/100, step 22/110, loss = 0.6847\n",
      "epoch 78/100, step 44/110, loss = 0.6838\n",
      "epoch 78/100, step 66/110, loss = 0.6817\n",
      "epoch 78/100, step 88/110, loss = 0.6854\n",
      "epoch 78/100, step 110/110, loss = 0.6839\n",
      "epoch 79/100, step 22/110, loss = 0.6878\n",
      "epoch 79/100, step 44/110, loss = 0.6847\n",
      "epoch 79/100, step 66/110, loss = 0.6859\n",
      "epoch 79/100, step 88/110, loss = 0.6847\n",
      "epoch 79/100, step 110/110, loss = 0.6856\n",
      "epoch 80/100, step 22/110, loss = 0.6818\n",
      "epoch 80/100, step 44/110, loss = 0.6823\n",
      "epoch 80/100, step 66/110, loss = 0.6826\n",
      "epoch 80/100, step 88/110, loss = 0.6800\n",
      "epoch 80/100, step 110/110, loss = 0.6930\n",
      "epoch 81/100, step 22/110, loss = 0.6799\n",
      "epoch 81/100, step 44/110, loss = 0.6844\n",
      "epoch 81/100, step 66/110, loss = 0.6834\n",
      "epoch 81/100, step 88/110, loss = 0.6834\n",
      "epoch 81/100, step 110/110, loss = 0.6810\n",
      "epoch 82/100, step 22/110, loss = 0.6796\n",
      "epoch 82/100, step 44/110, loss = 0.6867\n",
      "epoch 82/100, step 66/110, loss = 0.6825\n",
      "epoch 82/100, step 88/110, loss = 0.6877\n",
      "epoch 82/100, step 110/110, loss = 0.6765\n",
      "epoch 83/100, step 22/110, loss = 0.6833\n",
      "epoch 83/100, step 44/110, loss = 0.6822\n",
      "epoch 83/100, step 66/110, loss = 0.6811\n",
      "epoch 83/100, step 88/110, loss = 0.6844\n",
      "epoch 83/100, step 110/110, loss = 0.6898\n",
      "epoch 84/100, step 22/110, loss = 0.6902\n",
      "epoch 84/100, step 44/110, loss = 0.6798\n",
      "epoch 84/100, step 66/110, loss = 0.6822\n",
      "epoch 84/100, step 88/110, loss = 0.6853\n",
      "epoch 84/100, step 110/110, loss = 0.6856\n",
      "epoch 85/100, step 22/110, loss = 0.6819\n",
      "epoch 85/100, step 44/110, loss = 0.6819\n",
      "epoch 85/100, step 66/110, loss = 0.6828\n",
      "epoch 85/100, step 88/110, loss = 0.6813\n",
      "epoch 85/100, step 110/110, loss = 0.6804\n",
      "epoch 86/100, step 22/110, loss = 0.6834\n",
      "epoch 86/100, step 44/110, loss = 0.6777\n",
      "epoch 86/100, step 66/110, loss = 0.6827\n",
      "epoch 86/100, step 88/110, loss = 0.6823\n",
      "epoch 86/100, step 110/110, loss = 0.6817\n",
      "epoch 87/100, step 22/110, loss = 0.6771\n",
      "epoch 87/100, step 44/110, loss = 0.6769\n",
      "epoch 87/100, step 66/110, loss = 0.6780\n",
      "epoch 87/100, step 88/110, loss = 0.6826\n",
      "epoch 87/100, step 110/110, loss = 0.6761\n",
      "epoch 88/100, step 22/110, loss = 0.6806\n",
      "epoch 88/100, step 44/110, loss = 0.6862\n",
      "epoch 88/100, step 66/110, loss = 0.6780\n",
      "epoch 88/100, step 88/110, loss = 0.6830\n",
      "epoch 88/100, step 110/110, loss = 0.6822\n",
      "epoch 89/100, step 22/110, loss = 0.6828\n",
      "epoch 89/100, step 44/110, loss = 0.6865\n",
      "epoch 89/100, step 66/110, loss = 0.6739\n",
      "epoch 89/100, step 88/110, loss = 0.6777\n",
      "epoch 89/100, step 110/110, loss = 0.6825\n",
      "epoch 90/100, step 22/110, loss = 0.6752\n",
      "epoch 90/100, step 44/110, loss = 0.6856\n",
      "epoch 90/100, step 66/110, loss = 0.6872\n",
      "epoch 90/100, step 88/110, loss = 0.6837\n",
      "epoch 90/100, step 110/110, loss = 0.6794\n",
      "epoch 91/100, step 22/110, loss = 0.6858\n",
      "epoch 91/100, step 44/110, loss = 0.6765\n",
      "epoch 91/100, step 66/110, loss = 0.6786\n",
      "epoch 91/100, step 88/110, loss = 0.6791\n",
      "epoch 91/100, step 110/110, loss = 0.6823\n",
      "epoch 92/100, step 22/110, loss = 0.6736\n",
      "epoch 92/100, step 44/110, loss = 0.6865\n",
      "epoch 92/100, step 66/110, loss = 0.6824\n",
      "epoch 92/100, step 88/110, loss = 0.6800\n",
      "epoch 92/100, step 110/110, loss = 0.6787\n",
      "epoch 93/100, step 22/110, loss = 0.6817\n",
      "epoch 93/100, step 44/110, loss = 0.6830\n",
      "epoch 93/100, step 66/110, loss = 0.6687\n",
      "epoch 93/100, step 88/110, loss = 0.6737\n",
      "epoch 93/100, step 110/110, loss = 0.6770\n",
      "epoch 94/100, step 22/110, loss = 0.6806\n",
      "epoch 94/100, step 44/110, loss = 0.6694\n",
      "epoch 94/100, step 66/110, loss = 0.6829\n",
      "epoch 94/100, step 88/110, loss = 0.6811\n",
      "epoch 94/100, step 110/110, loss = 0.6785\n",
      "epoch 95/100, step 22/110, loss = 0.6762\n",
      "epoch 95/100, step 44/110, loss = 0.6799\n",
      "epoch 95/100, step 66/110, loss = 0.6830\n",
      "epoch 95/100, step 88/110, loss = 0.6716\n",
      "epoch 95/100, step 110/110, loss = 0.6681\n",
      "epoch 96/100, step 22/110, loss = 0.6720\n",
      "epoch 96/100, step 44/110, loss = 0.6721\n",
      "epoch 96/100, step 66/110, loss = 0.6773\n",
      "epoch 96/100, step 88/110, loss = 0.6792\n",
      "epoch 96/100, step 110/110, loss = 0.6679\n",
      "epoch 97/100, step 22/110, loss = 0.6733\n",
      "epoch 97/100, step 44/110, loss = 0.6780\n",
      "epoch 97/100, step 66/110, loss = 0.6653\n",
      "epoch 97/100, step 88/110, loss = 0.6773\n",
      "epoch 97/100, step 110/110, loss = 0.6833\n",
      "epoch 98/100, step 22/110, loss = 0.6840\n",
      "epoch 98/100, step 44/110, loss = 0.6718\n",
      "epoch 98/100, step 66/110, loss = 0.6775\n",
      "epoch 98/100, step 88/110, loss = 0.6775\n",
      "epoch 98/100, step 110/110, loss = 0.6778\n",
      "epoch 99/100, step 22/110, loss = 0.6699\n",
      "epoch 99/100, step 44/110, loss = 0.6728\n",
      "epoch 99/100, step 66/110, loss = 0.6674\n",
      "epoch 99/100, step 88/110, loss = 0.6696\n",
      "epoch 99/100, step 110/110, loss = 0.6845\n",
      "epoch 100/100, step 22/110, loss = 0.6824\n",
      "epoch 100/100, step 44/110, loss = 0.6635\n",
      "epoch 100/100, step 66/110, loss = 0.6668\n",
      "epoch 100/100, step 88/110, loss = 0.6642\n",
      "epoch 100/100, step 110/110, loss = 0.6756\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (features, labels) in enumerate(train_loader):\n",
    "        features = features.to(device)\n",
    "        labels = labels.to(device)\n",
    "        labels = labels.view(-1, 1)\n",
    "        # forward\n",
    "        output = model(features)\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if ((i+1) % 22 == 0):\n",
    "            print(f'epoch {epoch+1}/{num_epochs}, step {i+1}/{n_total_steps}, loss = {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score Naive Bayes untag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct = 1984/3509\n",
      "Accuracy = 56.54%\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for features, labels in test_loader:\n",
    "        features = features.to(device)\n",
    "        labels = labels.to(device)\n",
    "        labels = labels.view(-1, 1)  # Make sure labels are in the same shape as output\n",
    "        output = model(features)\n",
    "        \n",
    "        # Use sigmoid to get probabilities for binary classification\n",
    "        predictions = (output >= 0.5).float()  # Round the output (sigmoid result) to 0 or 1\n",
    "        \n",
    "        n_samples += labels.shape[0]\n",
    "        n_correct += (predictions == labels).sum().item()\n",
    "        \n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    \n",
    "    print(f'correct = {n_correct}/{n_samples}')\n",
    "    print(f'Accuracy = {acc:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model/pytorch.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4f/xq7119ld627_2wyd6dkmwl0m0000gn/T/ipykernel_23704/3079005891.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('model/pytorch.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralNet(\n",
       "  (l1): Linear(in_features=15474, out_features=100, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (l2): Linear(in_features=100, out_features=200, bias=True)\n",
       "  (relu2): LeakyReLU(negative_slope=0.01)\n",
       "  (l3): Linear(in_features=200, out_features=200, bias=True)\n",
       "  (relu3): LeakyReLU(negative_slope=0.01)\n",
       "  (l4): Linear(in_features=200, out_features=100, bias=True)\n",
       "  (relu4): ReLU()\n",
       "  (l5): Linear(in_features=100, out_features=100, bias=True)\n",
       "  (relu5): LeakyReLU(negative_slope=0.01)\n",
       "  (l6): Linear(in_features=100, out_features=50, bias=True)\n",
       "  (relu6): ReLU()\n",
       "  (l7): Linear(in_features=50, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (batch_norm1): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batch_norm2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the model again (must match the saved architecture)\n",
    "model = NeuralNet(input_size=input_size, hidden_size=hidden_size)\n",
    "\n",
    "# Load the state_dict into the model\n",
    "model.load_state_dict(torch.load('model/pytorch.pth'))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
